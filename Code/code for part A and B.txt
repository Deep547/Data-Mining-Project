##Createa dataset that only includes the versicolor and setosa species of iris
data(iris)#load the iris dataset
irisSubset<-iris[iris$Species!="virginica",c(1,3,5)]#Createa subset of the iris data such that Species Setosa and Versi color are fetched.
names(irisSubset)<-c("sepal","petal","species")#specifying the names of the columns
#pairs(iris[,1:4],col=iris$Species)
irisSubset2 <- irisSubset[sample(nrow(irisSubset)),] # Shuffling all the rows row-wise to randomize the data



# Predicting the class
distance.from.plane = function(z,weight,bias) {
 sum(z*weight) + bias
 }

classify.linear = function(x,weight,bias) {
distances = apply(x, 1, distance.from.plane, weight, bias)
return(ifelse(distances < 0, -1, +1))
 }

# Perceptron implementation 
#The basic algorithm is, assuming the separator passes through the origin and that the training labels Y i are either −1 or +1:
#initialize w = 0,while any training observation (~ x,Y) is not classified correctly set weight = weight + Yx
#Incorrectly classified training vectors are simply added to (or subtracted from) the separator normal, weight.
euclidean.norm=function(x){sqrt(sum(x*x))}
perceptron=function(x,y,learning.rate){
weight=vector(length=ncol(x))#initialize w
bias=0#Initialize b
k=0#count updates
R=max(apply(x,1,euclidean.norm))
made.mistake=TRUE#to enter the while loop
while(made.mistake){
made.mistake=FALSE#hopefully
yc<-classify.linear(x,weight,bias)
for(i in 1:nrow(x)){
if(y[i]!=yc[i]){
weight<-weight+learning.rate*y[i]*x[i,]
bias<-bias+learning.rate*y[i]*R^2
k<-k+1
made.mistake=TRUE
}
}}
s=euclidean.norm(weight)
return(list(weight=weight/s,bias=bias/s,updates=k))
}

# select the Sepal.Width versus Petal.Width scatter plot
x <- cbind(irisSubset$sepal,irisSubset$petal)
# label setosa as positive and the rest as negative
Y <- ifelse(irisSubset$species == "setosa", +1, -1)
# plot all the points
plot(x,cex=0.2,xlab="Sepal Length", ylab="Petal Length",main="Scatter of Sepal Length versus Petal Length for setosa versus specie versicolor" )
# use plus sign for setosa points
points(subset(x,Y==1),col="black",pch="+",cex=2)
# use minus sign for the rest
points(subset(x,Y==-1),col="red",pch="-",cex=2)
( p <- perceptron(x,Y,1) )
sum(abs(classify.linear(x,p$weight,p$bias) - Y))
# compute intercept on y axis of separator from weight and bias
intercept <- - p$bias / p$weight[[2]]
slope <- - p$weight[[1]] /p$weight[[2]]
abline(intercept,slope,col="green")


#Plotting the scatter plot when the data is shuffled
# select the Sepal.Width versus Petal.Width scatter plot
x <- cbind(irisSubset2$sepal,irisSubset2$petal)
# label setosa as positive and the rest as negative
Y <- ifelse(irisSubset2$species == "setosa", +1, -1)
# plot all the points
plot(x,cex=0.2,xlab="Sepal Length", ylab="Petal Length",main="Scatter of Sepal Length versus Petal Length for setosa versus specie versicolor" )
# use plus sign for setosa points
points(subset(x,Y==1),col="black",pch="+",cex=2)
# use minus sign for the rest
points(subset(x,Y==-1),col="red",pch="-",cex=2)
( p <- perceptron(x,Y,1) )
sum(abs(classify.linear(x,p$weight,p$bias) - Y))
# compute intercept on y axis of separator from weight and bias
intercept <- - p$bias / p$weight[[2]] 
slope <- - p$weight[[1]] /p$weight[[2]] #Weight determines the slope
abline(intercept,slope,col="green") 






PART B: Question 3
ai <- read.csv("C:/Users/dkoec/downloads/ai2013_papers (4).csv")#Load the file 
ai$type<-NULL # remove the column type form the dataset
ai$file<-NULL # remove the column file form the dataset

#Applying the kmeans clustering
results<- kmeans(ai,8) # run k means on the dataset to make 8 clusters, as the class variable type has 8 different types
results # shows the output/results regarding the clusters.
ai <- read.csv("C:/Users/dkoec/downloads/ai2013_papers (4).csv")
table(ai$type,results$cluster)# To check how the clustering did with respect to the real data.

#Check how accurate the cluster groupings are:
plot(ai[c("Res","Bac")],col=results$cluster)#Plotting the clusters from the result variable 
plot(ai[c("Res","Bac")],col=ai$type)# Plotting the clusters based on the actual dataset types column 


#Applying the Hierarchical Clustering
ai_num<-ai # assigning the ai dataset to ai_num 
ai_num$type<-NULL# dropping the type column from the dataset
ai_num$file<-NULL# dropping the file column from the dataset
clust_results<- hclust(dist(ai_num))#Hierarchical cluster analysis for the ai_num dataset on the basis of distance and assigning the results to clust_results. 
plot(clust_results,cex = 0.3)# Plotting the cluster_results generating a dendogram
clusterCut<- cutree(cluster_results,3)#cut the tree at the desired number of clusters(here 3) using cutree
table(clusterCut, ai$type) #Then we compare results with the original document types


#Applying the DBScan Clustering:
ai <- read.csv("C:/Users/dkoec/downloads/ai2013_papers (4).csv")#Load the file 
ai$type<-NULL # remove the column type form the dataset
ai$file<-NULL # remove the column file form the dataset
ai_mat<-as.matrix(ai) #Convert the data frame into a matrix
kNNdistplot(ai_mat, k=4)# For determining the value of epsilon for dbscan we use the k-nearest neighbor distances in a matrix of points. 
abline(h=150, col="red")# It can be seen that the optimal e value is around a distance of 160
set.seed(1234) #The seed number you choose is the starting point used in the generation of a sequence of random numbers
fpc <- fpc::dbscan(ai_mat, eps = 155, MinPts = 4)
db <- dbscan::dbscan(ai_mat, 155, 4)
hullplot(ai_mat,db$cluster)
ai <- read.csv("C:/Users/dkoec/downloads/ai2013_papers (4).csv")#Load the file
table(db$cluster,ai$type) #Then we compare results with the original document types

