	#=============================================================================
	# Importing the various required packages
	#=============================================================================
library(tm)             # package for text mining
library(stringr)        # package to find strings within data frame
library(SnowballC)      # package for stemming
library(slam)           # package for computation on sparse matrices
library(RWeka)          # package to use bigrams
library(topicmodels)    # package for LDA
library(randomForest)   # package for the random forests
library(fpc)            # package for DBSCAN, PAMK, and further clustering
library(cluster)        # package for PAM, silhouette measures
library(e1071)          # package for classifiers i.e. NB & SVM in this case

  #=============================================================================
  # Fetching the data and doing the preprocessing
  #=============================================================================
  
  # import data
  rdata <- read.csv("C:/Users/dkoec/Downloads/reutersCSV.csv", stringsAsFactors = FALSE)
  
  #-----------------------------------------------------------------------------
  # function to combine document title & text as an individual string, and to fetch organization id and doing basic word processing
  proc_data <- function(data) {
    
    #combining the text of document & title into one string
    data$doc <- paste(data$doc.title, data$doc.text, sep = " ")
    
    # extracting the id of the organizations
    data$doc <- gsub("&lt;", "<", data$doc)
    
    # replacing non-apostrophe punctuation with spaces
    data$doc <- gsub("[^[:alnum:][:space:]<>]", " ", data$doc)
    
    # converting output obtained from above steps into characters
    data <- data.frame(lapply(data, as.character), stringsAsFactors = FALSE)
  
  # fetching the organization id as single string
  org_id <- lapply(str_extract_all(data$doc, "(<.+?>)"), paste, collapse = " ")
  org_id <- gsub(" ", "", org_id)       # remove strings from organization id
  org_id <- gsub("><", "> <", org_id)   # replace spaces between organization id
  
 # removing duplicate organization identity from main doc
  data$doc <- str_replace_all(data$doc, "(<.+?>)", "")  
  
  # basic word pre-processing
  data$doc <- tolower(data$doc)
  data$doc <- removePunctuation(data$doc)
  data$doc <- removeNumbers(data$doc)
  data$doc <- removeWords(data$doc, stopwords("english"))
  data$doc <- stripWhitespace(data$doc)
  
  cbind.data.frame(data, org_id, stringsAsFactors = FALSE)
}

rdata <- proc_data(rdata)


	#=============================================================================
	# Document tagging with respect to the topics available
	#=============================================================================

# tag the topics
fetch_topics <- function(data, topic_list) {
  
  # output is stored in the dataframe
  tag <- data.frame()
  
  # looping over columns 4 to 138 for fetching documents with that tag applied
  for (i in 4:138) {
    
    # ignoring the untagged columns
    if (sum(as.numeric(data[,i])) > 0) {
      
      # fetching topic name from column header
      topic_name <- gsub("topic.", "", colnames(data)[i], )
      
      # tag the topics of interest
      if (topic_name %in% topic_list) {
        
        ntag <- cbind("pid" = data[data[,i] == 1, 1], 
                             "topic" = topic_name)
        
        tag <- rbind(tag, ntag)
      }
    }
  }
  
  # extract all documents that do not relate to topics on the list and tag as 'Other'
  dtag <- cbind("pid" = data[!data$pid %in% tag$pid, 1],
                    "topic" = "_other")
  
  # generate entire data with assigned topic tags 
  data.frame(lapply(rbind(tag, dtag), as.character), stringsAsFactors = F)
}

desired_topics <- c("earn", "acq", "money.fx", "grain", "crude", "trade", "interest", "ship", "wheat", "corn")
topic_list <- fetch_topics(rdata, desired_topics)

#-----------------------------------------------------------------------------
# cleaning up duplicate grain or corn or wheat tags to allow for better separation

clear_dup <- function(data, topics.to.keep, topics.to.cut) {
  
  # extracting documents with corn, wheat or grain as topic
  cowh <- data[data$topic %in% c(topics.to.keep), ]
  gr <- data[data$topic %in% c(topics.to.cut), ]
  
  # creating df from all corn or wheat topics and all 'grain' not also tag as corn or wheat
  cowhg <- rbind(cowh, gr[!(gr$pid %in% cowh$pid), ])
  
  # trimmming data to remove all lines with topics in question
  data <- data[!(data$topic %in% c(topics.to.keep, topics.to.cut)), ]
  
  # reattaching topics in question without duplicated lines
  rbind(data, cowhg)
}

# trimming data so that topics tagged as "grain" as well as "corn" or "wheat" are tag only as "corn" or "wheat"
topic_list <- clear_dup(topic_list, c("corn", "wheat"), c("grain"))


	#=============================================================================
	# Creating the document term matrix
	#=============================================================================

#  bigram tokenizer: includes both unigrams & bigrams
bigram_tokenize <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 2))

# create a DTM of unigrams & bigrams
dtm <- DocumentTermMatrix(Corpus(VectorSource(rdata$doc)),
                              control = list(tokenize = scan_tokenizer,
                                             stemming = T))

#  As LDA does not work well with sparse terms so we remove sparse terms
dtm <- removeSparseTerms(dtm, .95)

# replacing document number with PID
dtm$dimnames$Docs <- rdata$pid


	#=============================================================================
	# Generating the topic models
	#=============================================================================

# for training LDA, removing 'noise' documents from training set
lda_dtm <- rdata$pid[rdata$pid %in% c(rdata$pid[rdata$purpose == "test"],
                                          topic_list$pid[topic_list$topic %in% desired_topics])]

lda_dtm <- dtm[dtm$dimnames$Docs %in% lda_dtm,]

# removing empty rows
lda_dtm <- lda_dtm[row_sums(lda_dtm) > 0, ]

# running latent dirichlet allocation to generate 10 topics
lda <- LDA(lda_dtm, 10)

# getting probabilities to use as features - reattach to list of all PIDs
lda_prob <- merge(rdata[,c(1,3)], 
                   cbind.data.frame("pid" = lda@documents, lda@gamma, stringsAsFactors = F),
                   by = "pid", all.x = T, incomparables = 0)

# replacing unmatched values with 0 instead of NA: uniform non-zero likelihood of each class
lda_prob[is.na(lda_prob)] <- 0


	#=============================================================================
	#Single labeled feature matrix
	#=============================================================================

# combining DTM, topic tags and LDA probabilities into data frame matching on pid
total_data <- merge(topic_list, cbind(lda_prob, data.matrix(dtm)), by = "pid", all.x = "T")

# assuring topic must be a factor for classification
total_data$topic <- as.factor(total_data$topic)

# extracting training set
train_data <- total_data[total_data$purpose != "test", -c(1,3)]


	#=============================================================================
	#  Implementing some additional feature sets
	#=============================================================================

# getting the n most likely terms per LDA topic cluster
lda10 <- unique(c(terms(lda, 10)))
lda20 <- unique(c(terms(lda, 20)))

#-----------------------------------------------------------------------------

# extracting the n most frequent terms per topic
freq_terms <- function(dtm, n = 10) {
  
  terms <- list()
  
  # filtering DTM to select terms for one topic at a time
  for (i in 1:length(levels(dtm$topic))) {
    topic <- levels(dtm$topic)[i]
    
    t.dtm <- dtm[dtm$topic == topic, -1]
    
    # selecting terms with highest term frequency
    terms[[i]] <- names(sort(colSums(t.dtm), decreasing = T)[1:n])
    names(terms)[i] <- topic
  }
  
  terms
}

fr_10 <- unique(unlist(freq_terms(train_data[,-(2:11)], 10)))
fr_20 <- unique(unlist(freq_terms(train_data[,-(2:11)], 20)))

#-----------------------------------------------------------------------------
# Function to fetch the n most significant terms per topic by Chi-Squared value
chi_sq_terms <- function(dtm, n = 10) {
  
  terms <- list()
  
  # iterating over each topic, summing for each term
  for (i in 1:length(levels(dtm$topic))) {
    topic <- levels(dtm$topic)[i]
    cs <- c()
    
    tab <- rbind(colSums(dtm[dtm$topic == topic, -1]),
                 colSums(dtm[dtm$topic != topic, -1]))
    
    # iterating over each term, calculating Chi-sq statistic for each
    for (j in 1:ncol(tab)) {
      cs[j] <- suppressWarnings(chisq.test(cbind(tab[,j], rowSums(tab[,-j])))$statistic)
    }
    
    # allocating terms to vector of Chi-sq Ress
    names(cs) <- colnames(dtm)[-1]
    
    # taking top n terms by Chi-sq, allocate to 'terms' list
    terms[[i]] <- names(sort(cs, decreasing = T))[1:n]
  }
  names(terms) <- levels(dtm$topic)
  terms
}

cs_10 <- unique(unlist(chi_sq_terms(train_data[,-(2:11)], 10)))
cs_20 <- unique(unlist(chi_sq_terms(train_data[,-(2:11)], 20)))


	#=============================================================================
	# Accessing the performance for the first run by implementing functions
	#=============================================================================

# implementing function to calculate performance measures per class
perf_assess <- function(res) {
  
  cm <- table("Expected" = res$Expected, "predicted" = res$predicted)
  
  if (ncol(cm) != nrow(cm)) {
    missing <- rownames(cm)[!(rownames(cm) %in% colnames(cm))]
    m <- matrix(nrow = nrow(cm), ncol = length(missing), 0)
    colnames(m) <- missing
    cm <- cbind(cm, m)
    cm <- cm[, sort(colnames(cm))]
  }
  
  TP <- diag(cm)
  FN <- rowSums(cm) - TP
  FP <- colSums(cm) - TP
  TN <- sum(cm) - TP - FN - FP
  
  # removing values for 'other'
  TP <- TP[-1]; FN <- FN[-1]; FP <- FP[-1]; TN <- TN[-1]
  
  Recall <- TP / (TP + FN)
  Precision <- TP / (TP + FP)
  Fmeasure <- (2 * Recall * Precision) / (Recall + Precision)
  Accuracy <- (TP + TN) / (TP + TN + FN + FP)
  
  tab <- cbind(Recall, Precision, Fmeasure, Accuracy)
  tab[is.na(tab)] <- 0
  
  # adding macro-averages
  tab <- rbind(tab, "macro.avg" = colSums(tab) / nrow(tab))
  
  # calculating micro-averaged performance
  micro.Recall <- sum(TP) / (sum(TP) + sum(FN))
  micro.Precision <- sum(TP) / (sum(TP) + sum(FP))
  micro.Fmeasure <- (2 * micro.Recall * micro.Precision) / (micro.Recall + micro.Precision)
  micro.Accuracy <- (sum(TP) + sum(TN)) / (sum(TP) + sum(TN) + sum(FN) + sum(FP))
  
  tab <- rbind(tab, "micro.avg" = cbind(micro.Recall, micro.Precision,
                                        micro.Fmeasure, micro.Accuracy))
  rownames(tab)[nrow(tab)] <- "micro.avg"
  tab
}


# implementing first pass: split 80/20 into temporary training and test sets
# for a quick initial comparison of feature sets
Quick_Svm <- function(data, features) {
  
  # trimming data to only required features
  data <- data[,colnames(data) %in% c("topic", c(1:10), features)]
  
  # randomly ordering the data
  rand <- data[sample(nrow(data), replace = FALSE),]
  
  # splitting 80:20 into training & test sets
  train <- rand[1:floor(nrow(rand) * .8), ]
  test <- rand[-(1:floor(nrow(rand) * .8)), ]
  
  # removing the empty rows & columns from training data
  train <- train[rowSums(train[,-1]) > 0, ]
  train <- train[,!(colnames(train) %in% colnames(train[,-1])[colSums(train[,-1]) == 0])]
  
  # running Model & comparing Predictions to expected values
  Model <- svm(topic ~ ., data = train)
  Prediction <- predict(Model, newdata = test[,-1])
  Res <- as.data.frame(cbind("Expected" = as.character(test[,1]), "predicted" = as.character(Prediction)))
  perf <- round(perf_assess(Res)*100,1)
  
  z <- c(length(colnames(data)) - 1, perf[11:12,3:4], Model$kernel, Model$cost)
  names(z) <- c("n.features", "macro-f", "micro-f", "macro-a", "micro-a", "kernel", "cost")
  list("call" = Model$call, "perf" = z, "cm" = table(Res))
}

	#=============================================================================
	# Checking the performance across extra feature sets
	#=============================================================================

proposed.features <- list('topic Models only' = "", '+ lda10' = lda10, '+ lda20' = lda20, 
                          '+ fr_10' = fr_10, '+ fr_20' = fr_20,
                          '+ cs_10' = cs_10, '+ cs_20' = cs_20,
                          '+ all terms' = dtm$dimnames$Terms)

# running the quick SVM Model over 80:20 split of training data for comparing performance
feature.comparison <- list()
for (i in 1:length(proposed.features)) {
  features <- proposed.features[[i]]
  feature.comparison[[i]] <- Quick_Svm(train_data, proposed.features[[i]])
  names(feature.comparison)[i] <- names(proposed.features)[i]
}
# extracting performance measures & save to csv
feature.scores <- t(sapply(feature.comparison, "[[", 2))
feature.scores <- cbind.data.frame("feature.set" = rownames(feature.scores), feature.scores, stringsAsFactors = F)

write.csv(feature.scores, file = "feature.comparison.csv")


	#=============================================================================
	# Checking Performance making use of K-Fold Cross-Evaluation
	#=============================================================================

# function to run & evaluate k-fold cross-validation
run.kfold <- function(dtm, k = 10, classifier = svm) {
  
  # sorting data into random order
  dtm <- dtm[sample(nrow(dtm), replace = FALSE), ]
  
  # creating vector containing fold number
  folds <- rep(1:k, ceiling(nrow(dtm) / k))[1:nrow(dtm)]
  
  # initializing data frames to hold predicted & Expected topics
  Prediction <- data.frame()
  Expected <- data.frame()
  
  # iterating over all k folds, each time reserving one to evaluate and using the rest to train
  for (i in 1:k){
    
    # splitting the data into test & training sets
    trainingset <- dtm[folds != i,]
    
    # cleaning training set - removing any empty rows or columns
    trainingset <- trainingset[rowSums(trainingset[,-1]) > 0, ]
    trainingset <- trainingset[,!(colnames(trainingset) %in% colnames(trainingset[,-1])[colSums(trainingset[,-1]) == 0])]
    
    testset <- dtm[folds == i,]
    
    # running classifier on the training data
    Model <- classifier(topic ~ ., data = trainingset)
    
    # using the classifier to predict classes on test set (excluding actual topic)
    temp <- as.data.frame(predict(Model, newdata = testset[,-1]))
    
    # adding this iteration's Predictions to 'Prediction' data frame
    Prediction <- rbind(Prediction, temp)
    
    # adding correct classifications to 'Expected' data frame
    Expected <- rbind(Expected, as.data.frame(testset[, 1]))
  }
  
  Res <- cbind(Expected, Prediction, folds, Prediction == Expected)
  colnames(Res) <- c("Expected", "predicted", "fold", "correct")
  
  perf <- round(perf_assess(Res)*100,1)
  
  m <- round(mean(table(Res$fold, Res$correct)[,2]/rowSums(table(Res$fold, Res$correct)))*100, 1)
  s <- round(sd(table(Res$fold, Res$correct)[,2]/rowSums(table(Res$fold, Res$correct))), 3)
  
  z <- c(length(colnames(dtm)) - 1, perf[11:12,3:4], m, s)
  names(z) <- c("n.features", "macro-f", "micro-f", "macro-a", "micro-a", "pf-mean", "pf-sd")
  list("call" = Model$call, "perf" = z, "Res" = Res)
}

kfold.fr10 <- run.kfold(train_data[,colnames(train_data) %in% c("topic", c(1:10), fr_10)])


	#=============================================================================
	# Executing the same feature set with normalized frequencies
	#=============================================================================

# normalizing the frequencies by dividing by max. frequency of that term
training.norm <- cbind(train_data[,1:11],
                       sweep(train_data[,-(1:11)], 2, sapply(train_data[,-(1:11)], max), '/'))

kfold.fr10.norm <- run.kfold(training.norm[,colnames(training.norm) %in% c("topic", c(1:10), fr_10)])


	#=============================================================================
	# Executing the same feature set with binary frequencies
	#=============================================================================

# converting to binary vector instead of frequencies
training.bin <- cbind(train_data[,1:11],
                      ceiling(train_data[,-(1:11)] / (train_data[,-(1:11)] + 1)))

kfold.fr10.bin <- run.kfold(training.bin[,colnames(training.bin) %in% c("topic", c(1:10), fr_10)])

classifier_kfold <- rbind(cbind('features' = 'topic Models + fr_10', 
                                 data.frame(t(kfold.fr10$perf))),
                           cbind('features' = 'topic Models + fr_10, normalised', 
                                 data.frame(t(kfold.fr10.norm$perf))),
                           cbind('features' = 'topic Models + fr_10, binary', 
                                 data.frame(t(kfold.fr10.bin$perf))))

write.csv(classifier_kfold, "kfold.feature.selection.csv")

# fixing the final feature set
features <- c(fr_10, c(1:10))

# cutting down training set to contain only final feature set
train_data <- train_data[,colnames(train_data) %in% c("topic", c(1:10), fr_10)]

# making sure that the column names are not numeric else the random forest classifier will fail
colnames(train_data)[2:11] <- c('v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10')


	#=============================================================================
	# Executing the selected feature set with respect to the classifiers
	#=============================================================================

kfold.rf <- run.kfold(train_data, classifier = randomForest)
kfold.nb <- run.kfold(train_data, classifier = naiveBayes)

	#=============================================================================
	# SVM Parameters Tuning
	#=============================================================================

# an extra function for tuning SVM paramaters (uses linear kernel by default)
tune.kfold.svm <- function(dtm, k = 10, c = 1) {
  
  # sorting the data into random order
  dtm <- dtm[sample(nrow(dtm), replace = FALSE), ]
  
  # creating vector containing fold number
  folds <- rep(1:k, ceiling(nrow(dtm) / k))[1:nrow(dtm)]
  
  # initializing data frames to hold predicted & Expected topics
  Prediction <- data.frame()
  Expected <- data.frame()
  
  # iterating over all k folds, each time reserving one to evaluate and using the rest to train
  for (i in 1:k){
    
    # spliting data into test & training sets
    trainingset <- dtm[folds != i,]
    
    # cleaning training set - remove any empty rows or columns
    trainingset <- trainingset[rowSums(trainingset[,-1]) > 0, ]
    trainingset <- trainingset[,!(colnames(trainingset) %in% colnames(trainingset[,-1])[colSums(trainingset[,-1]) == 0])]
    
    testset <- dtm[folds == i,]
    
    # running classifier on the training data
    Model <- svm(topic ~ ., data = trainingset, kernel = "linear", cost = c)
    
    # using classifier to predict classes on test set (excluding actual topic)
    temp <- as.data.frame(predict(Model, newdata = testset[,-1]))
    
    # adding this iteration's Predictions to 'Prediction' data frame
    Prediction <- rbind(Prediction, temp)
    
    # adding correct classifications to 'Expected' data frame
    Expected <- rbind(Expected, as.data.frame(testset[, 1]))
  }
  
  Res <- cbind(Expected, Prediction, folds, Prediction == Expected)
  colnames(Res) <- c("Expected", "predicted", "fold", "correct")
  
  perf <- round(perf_assess(Res)*100,1)
  
  m <- round(mean(table(Res$fold, Res$correct)[,2]/rowSums(table(Res$fold, Res$correct)))*100, 1)
  s <- round(sd(table(Res$fold, Res$correct)[,2]/rowSums(table(Res$fold, Res$correct))), 3)
  
  z <- c(length(colnames(dtm)) - 1, perf[11:12,3:4], m, s)
  names(z) <- c("n.features", "macro-f", "micro-f", "macro-a", "micro-a", "pf-mean", "pf-sd")
  list("call" = Model$call, "perf" = z, "Res" = Res)
}

# experimenting with changing svm parameters
kfold.svm.linear <- tune.kfold.svm(train_data)
kfold.svm.linear.5 <- tune.kfold.svm(train_data, c = 0.5)
kfold.svm.linear.1 <- tune.kfold.svm(train_data, c = 0.1)


	#=============================================================================
	#  RF Parameters Tuning
	#=============================================================================

tune.kfold.rf <- function(dtm, k = 10, m = 5) {
  
  # sorting data into random order
  dtm <- dtm[sample(nrow(dtm), replace = FALSE), ]
  
  # creating vector containing fold number
  folds <- rep(1:k, ceiling(nrow(dtm) / k))[1:nrow(dtm)]
  
  # initializing data frames to hold predicted & Expected topics
  Prediction <- data.frame()
  Expected <- data.frame()
  
  # iterating over all k folds, each time reserving one to evaluate and using the rest to train
  for (i in 1:k){
    
    # splitting data into test & training sets
    trainingset <- dtm[folds != i,]
    
    # cleaning training set - remove any empty rows or columns
    trainingset <- trainingset[rowSums(trainingset[,-1]) > 0, ]
    trainingset <- trainingset[,!(colnames(trainingset) %in% colnames(trainingset[,-1])[colSums(trainingset[,-1]) == 0])]
    
    testset <- dtm[folds == i,]
    
    # running classifier on the training data
    Model <- randomForest(topic ~ ., data = trainingset, mtry = m)
    
    # using classifier to predict classes on test set (excluding actual topic)
    tempo <- as.data.frame(predict(Model, newdata = testset[,-1]))
    
    # adding this iteration's Predictions to 'Prediction' data frame
    Prediction <- rbind(Prediction, tempo)
    
    # adding correct classifications to 'Expected' data frame
    Expected <- rbind(Expected, as.data.frame(testset[, 1]))
  }
  
  Res <- cbind(Expected, Prediction, folds, Prediction == Expected)
  colnames(Res) <- c("Expected", "predicted", "fold", "correct")
  
  perf <- round(perf_assess(Res)*100,1)
  
  m <- round(mean(table(Res$fold, Res$correct)[,2]/rowSums(table(Res$fold, Res$correct)))*100, 1)
  s <- round(sd(table(Res$fold, Res$correct)[,2]/rowSums(table(Res$fold, Res$correct))), 3)
  
  z <- c(length(colnames(dtm)) - 1, perf[11:12,3:4], m, s)
  names(z) <- c("n.features", "macro-f", "micro-f", "macro-a", "micro-a", "pf-mean", "pf-sd")
  list("call" = Model$call, "perf" = z, "Res" = Res)
}

kfold_rf_m4 <- tune.kfold.rf(train_data, m = 4)
kfold_rf_m6 <- tune.kfold.rf(train_data, m = 6)


classifier_kfold <- rbind(cbind('classifier' = 'SVM', data.frame(t(kfold.fr10$perf))),
                           cbind('classifier' = 'Random Forest', data.frame(t(kfold.rf$perf))),
                           cbind('classifier' = 'Naive Bayes', data.frame(t(kfold.nb$perf))),
                           cbind('classifier' = 'SVM (linear kernel)', data.frame(t(kfold.svm.linear$perf))),
                           cbind('classifier' = 'SVM (linear kernel, cost 0.5)', data.frame(t(kfold.svm.linear.5$perf))),
                           cbind('classifier' = 'SVM (linear kernel, cost 0.1)', data.frame(t(kfold.svm.linear.1$perf))),
                           cbind('classifier' = 'RF (mtry = 4)', data.frame(t(kfold_rf_m4$perf))),
                           cbind('classifier' = 'RF (mtry = 6)', data.frame(t(kfold_rf_m6$perf))))
colnames(classifier_kfold)[1] <- "classifier"

write.csv(classifier_kfold, file = "kfold.classifier.selection.csv")


	#=============================================================================
	#Training the classifier over entire training set and predicting the test set
	#=============================================================================

# extracting the training set
test_data <- total_data[total_data$purpose == "test", -c(1,3)]

# converting the column names to match those of the training set
colnames(test_data)[2:11] <- c('v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10')

# trimmng the test data to contain only columns of interest
test_data <- test_data[, colnames(test_data) %in% colnames(train_data)]

# training the Model and make Predictions
Model <- randomForest(topic ~ ., data = train_data, mtry = 6)
Prediction <- predict(Model, newdata = test_data[,-1])
Res <- as.data.frame(cbind("Expected" = as.character(test_data[,1]), "predicted" = as.character(Prediction)))
performance <- perf_assess(Res)

performance <- round(performance*100, 1)
performance <- cbind(rownames(performance), performance)
performance[11:12,1] <- c("Macro-average", "Micro-average")
colnames(performance) <- c("topic", "Recall", "Precision", "Fmeasure", "Accuracy")
write.csv(performance, file = "classifier-performance.csv")

# calculating the correlation between performance & topic size
cor(as.numeric(table(test_data$topic)[-1]), as.numeric(performance[1:10,2]))     # Recall: .823
cor(as.numeric(table(test_data$topic)[-1]), as.numeric(performance[1:10,3]))     # Precision: .878
cor(as.numeric(table(test_data$topic)[-1]), as.numeric(performance[1:10,4]))     # f-measure: .891
cor(as.numeric(table(test_data$topic)[-1]), as.numeric(performance[1:10,5]))     # Accuracy: -.756


# storing the output confusion matrix to .csv
cm <- as.data.frame.matrix(table("Expected" = Res$Expected, "predicted" = Res$predicted))
cm <- cbind.data.frame("Expected" = rownames(cm), cm, stringsAsFactors = F)
cm[cm == "_other"] <- "other"

write.csv(cm, file = "cm.csv")

	#=============================================================================
	# Implementation of Various Clustering Methods
	#=============================================================================

# stripping out topics from data, retain only unique records
total_data  <- unique(total_data[, -(2:3)])

# stripping out all but selected features
pid <- total_data[,1]
total_data <- total_data[, colnames(total_data) %in% c(c(1:10), fr_10)]

# getting distance matrix
dist_mat <- dist(total_data)

# getting all original topic tags from data (not just the top 10)
all_topics <- gsub("topic.", "", colnames(rdata)[4:138], )
org_topics <- fetch_topics(rdata, all_topics)

# function to label clusters according to main topic picked up
label_clusters <- function(clusters, topics) {
  
  # matching clusters to original topic labels via PID
  match <- merge(topics, clusters, by = "pid", all.x = T)
  
  # finding dominant topic in each cluster, excluding 'other'
  clust_cm <- table("Expected" = match$topic, "cluster" = match$cluster)
  c <- cbind("cluster" = colnames(clust_cm), 
             "predicted" = rownames(clust_cm[-1,])[apply(clust_cm[-1,],2,which.max)])
  
  # As the largest cluster will be 'noise' cluster - label this as '_other'
  c[c[,1] == names(colSums(clust_cm)[which.max(colSums(clust_cm))]),2] <- "_other"
  
  # returning the expected & predicted topics per document
  colnames(c) <- c("cluster", "clust.topic")
  z <- merge(match, c, by = "cluster", all.x = T)
  colnames(z) <- c("cluster", "pid", "Expected", "predicted")
  z
}


	#=============================================================================
	# PAM (Partitioning Around K-Medoids) implementation and analysis
	#=============================================================================

pam10 <- pam(total_data, k = 10)

# storing the output silhouette to pdf
pdf('pam10-silhouette.pdf');
plot(silhouette(pam10));
dev.off()

# assessing the performance of clustering against all 135 original topic tags
pam_clusters <- cbind("pid" = pid, "cluster" = pam10$cluster)
pam_clusters <- label_clusters(pam_clusters, org_topics)
pam_perf <- perf_assess(pam_clusters)

# saving an abridged version, only including clusters with Predictions
pam_abr <- cbind("topic" = rownames(pam_perf[pam_perf[,1] > 0, ]),
                 round(pam_perf[pam_perf[,1] > 0, ]*100, 1),
                 "nexp" = c(table(pam_clusters$Expected)[names(table(pam_clusters$Expected)) %in% rownames(pam_perf[pam_perf[,1] > 0, ])],0,0),
                 "npred" = c(table(pam_clusters$predicted)[names(table(pam_clusters$predicted)) %in% rownames(pam_perf[pam_perf[,1] > 0, ])],0,0))
write.csv(pam_abr, file = "pam-performance.csv")

	#=============================================================================
	# Hierarchical Clustering implementation and Analysis
	#=============================================================================

hc <- hclust(dist_mat)

# plotting hierarchical clustering dendrogram
pdf('hclust.pdf')
plot(hc, labels = F, xlab = "")
dev.off()


# creating confusion matrices & output to .csv
hclust_10_cm <- as.data.frame.matrix(table(merge(topic_list, cbind("pid" = pid, "cluster" = cutree(hc, k = 10)),
                                                 by = "pid", all.x = T)[,3:2]))
hclust_10_cm <- cbind.data.frame("cluster" = rownames(hclust_10_cm), hclust_10_cm, stringsAsFactors = F)
colnames(hclust_10_cm)[2] <- "other"
write.csv(hclust_10_cm, file = "hclust-10-cm.csv")

hclust_24_cm <- as.data.frame.matrix(table(merge(topic_list, cbind("pid" = pid, "cluster" = cutree(hc, h = 24)),
                                                 by = "pid", all.x = T)[,3:2]))
hclust_24_cm <- cbind.data.frame("cluster" = rownames(hclust_24_cm), hclust_24_cm, stringsAsFactors = F)
colnames(hclust_24_cm)[2] <- "other"
write.csv(hclust_24_cm, file = "hclust-24-cm.csv")

hclust_150_cm <- as.data.frame.matrix(table(merge(topic_list, cbind("pid" = pid, "cluster" = cutree(hc, k = 150)),
                                                  by = "pid", all.x = T)[,3:2]))
hclust_150_cm <- cbind.data.frame("cluster" = rownames(hclust_150_cm), hclust_150_cm, stringsAsFactors = F)
colnames(hclust_150_cm)[2] <- "other"
write.csv(hclust_150_cm, file = "hclust-150-cm.csv")


# plotting silhouette measures
hclust_10 <- silhouette(cutree(hc, k = 10), dist_mat)
hclust_24 <- silhouette(cutree(hc, h = 24), dist_mat)
hclust_150 <- silhouette(cutree(hc, k = 150), dist_mat)


pdf('hclust10-silhouette.pdf')
plot(hclust_10, do.n.k = F, do.clus.stat = F, main = "")
dev.off()

pdf('hclust24-silhouette.pdf')
plot(hclust_24, do.n.k = F, do.clus.stat = F, main = "")
dev.off()

pdf('hclust150-silhouette.pdf')
plot(hclust_150, do.n.k = F, do.clus.stat = F, main = "")
dev.off()


# plotting per-class silhouette measure without maximum class
pdf('hclust10-plot.pdf')
plot(summary(hclust_10)$clus.avg.widths, 
     summary(hclust_10)$clus.sizes, 
     ylim = c(0, max(summary(hclust_10)$clus.sizes[-which.max(summary(hclust_10)$clus.sizes)])), 
     pch = 16, ylab = "Cluster size", xlab = "Cluster silhouette measure (10 clusters)", cex = 2, oma = c(1,1,0,0))
dev.off()

pdf('hclust24-plot.pdf')
plot(summary(hclust_24)$clus.avg.widths, 
     summary(hclust_24)$clus.sizes, 
     ylim = c(0, max(summary(hclust_24)$clus.sizes[-which.max(summary(hclust_24)$clus.sizes)])), 
     pch = 16, ylab = "Cluster size", xlab = "Cluster silhouette measure (clusters at height 24)", cex = 2, oma = c(1,1,0,0))
dev.off()

pdf('hclust150-plot.pdf')
plot(summary(hclust_150)$clus.avg.widths, 
     summary(hclust_150)$clus.sizes, 
     ylim = c(0, max(summary(hclust_150)$clus.sizes[-which.max(summary(hclust_150)$clus.sizes)])), 
     pch = 16, ylab = "Cluster size", xlab = "Cluster silhouette measure (150 clusters)", cex = 2, oma = c(1,1,0,0))
dev.off()

# assessing the performance of clustering against all 135 original topic tags
hc10.clusters <- cbind("pid" = pid, "cluster" = cutree(hc, k = 10))
hc10.clusters <- label_clusters(hc10.clusters, org_topics)
hc10.perf <- perf_assess(hc10.clusters)

# saving an abridged version, only including clusters with Predictions
hc10.abr <- cbind("topic" = rownames(hc10.perf[hc10.perf[,1] > 0, ]),
                  round(hc10.perf[hc10.perf[,1] > 0, ]*100, 1),
                  "nexp" = c(table(hc10.clusters$Expected)[names(table(hc10.clusters$Expected)) %in% rownames(hc10.perf[hc10.perf[,1] > 0, ])],0,0),
                  "npred" = c(table(hc10.clusters$predicted)[names(table(hc10.clusters$predicted)) %in% rownames(hc10.perf[hc10.perf[,1] > 0, ])],0,0))
write.csv(hc10.abr, file = "hc10-performance.csv")


# assessing the performance of clustering against all 135 original topic tags
hc24.clusters <- cbind("pid" = pid, "cluster" = cutree(hc, h = 24))
hc24.clusters <- label_clusters(hc24.clusters, org_topics)
hc24.perf <- perf_assess(hc24.clusters)

# saving an abridged version, only including clusters with Predictions
hc24.abr <- cbind("topic" = rownames(hc24.perf[hc24.perf[,1] > 0, ]),
                  round(hc24.perf[hc24.perf[,1] > 0, ]*100, 1),
                  "nexp" = c(table(hc24.clusters$Expected)[names(table(hc24.clusters$Expected)) %in% rownames(hc24.perf[hc24.perf[,1] > 0, ])],0,0),
                  "npred" = c(table(hc24.clusters$predicted)[names(table(hc24.clusters$predicted)) %in% rownames(hc24.perf[hc24.perf[,1] > 0, ])],0,0))
write.csv(hc24.abr, file = "hc24-performance.csv")



# assessing the performance of clustering against all 135 original topic tags
hc150.clusters <- cbind("pid" = pid, "cluster" = cutree(hc, k = 150))
hc150.clusters <- label_clusters(hc150.clusters, org_topics)
hc150.perf <- perf_assess(hc150.clusters)

# saving an abridged version, only including clusters with Predictions
hc150.abr <- cbind("topic" = rownames(hc150.perf[hc150.perf[,1] > 0, ]),
                   round(hc150.perf[hc150.perf[,1] > 0, ]*100, 1),
                   "nexp" = c(table(hc150.clusters$Expected)[names(table(hc150.clusters$Expected)) %in% rownames(hc150.perf[hc150.perf[,1] > 0, ])],0,0),
                   "npred" = c(table(hc150.clusters$predicted)[names(table(hc150.clusters$predicted)) %in% rownames(hc150.perf[hc150.perf[,1] > 0, ])],0,0))
write.csv(hc150.abr, file = "hc150-performance.csv")

	#=============================================================================
	# DBSCAN Clustering implementation and analysis
	#=============================================================================

db <- dbscan(total_data, eps = 0.5, MinPts = 50, method = "raw")

dbclust <- silhouette(db$cluster, dist_mat)

pdf('dbclust-silhouette.pdf')
plot(dbclust, do.n.k = F, do.clus.stat = F, main = "")
dev.off()

# creating confusion matrix & output to .csv
dbclusts <- merge(org_topics, cbind("pid" = pid, "cluster" = db$cluster, by = "pid", all.x = T))
dbclust_cm <- as.data.frame.matrix(table(dbclusts[,3:2]))
dbclust_cm <- cbind.data.frame("cluster" = rownames(dbclust_cm), dbclust_cm, stringsAsFactors = F)
colnames(dbclust_cm)[2] <- "other"
write.csv(dbclust_cm, file = "dbclust-cm.csv")


# assessing performance of clustering against all 135 original topic tags
db.clusters <- cbind("pid" = pid, "cluster" = db$cluster)
db.clusters <- label_clusters(db.clusters, org_topics)
db.perf <- perf_assess(db.clusters)

# saving an abridged version, only including clusters with Predictions
db.abr <- cbind("topic" = rownames(db.perf[db.perf[,1] > 0, ]),
                round(db.perf[db.perf[,1] > 0, ]*100, 1),
                "nexp" = c(table(db.clusters$Expected)[names(table(db.clusters$Expected)) %in% rownames(db.perf[db.perf[,1] > 0, ])],0,0),
                "npred" = c(table(db.clusters$predicted)[names(table(db.clusters$predicted)) %in% rownames(db.perf[db.perf[,1] > 0, ])],0,0))

write.csv(db.abr, file = "db-performance.csv")